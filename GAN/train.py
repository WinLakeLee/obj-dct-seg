import tensorflow as tf
from tensorflow.keras import layers, Model, optimizers
import numpy as np
import os
import argparse
import matplotlib.pyplot as plt

# ==========================================
# 1. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (DCGAN êµ¬ì¡°)
# ==========================================
class AnomalyGAN:
    def __init__(self, input_shape, latent_dim, learning_rate):
        self.img_shape = input_shape
        self.latent_dim = latent_dim
        self.learning_rate = learning_rate
        
        self.optimizer = optimizers.Adam(self.learning_rate, 0.5)

        self.generator = self.build_generator()
        self.discriminator = self.build_discriminator()
        
        self.discriminator.compile(loss='binary_crossentropy', optimizer=self.optimizer, metrics=['accuracy'])
        self.gan = self.build_gan()

    def build_generator(self):
        model = tf.keras.Sequential(name="Generator")
        # ì…ë ¥ ì°¨ì› ê³„ì‚° (128x128 ê¸°ì¤€ 8x8ì—ì„œ ì‹œì‘)
        start_dim = self.img_shape[0] // 16 
        
        model.add(layers.Dense(start_dim * start_dim * 256, input_dim=self.latent_dim))
        model.add(layers.Reshape((start_dim, start_dim, 256)))
        model.add(layers.BatchNormalization())
        model.add(layers.LeakyReLU(alpha=0.2))

        # Upsampling block
        for filters in [128, 64, 32]:
            model.add(layers.Conv2DTranspose(filters, 4, strides=2, padding='same'))
            model.add(layers.BatchNormalization())
            model.add(layers.LeakyReLU(alpha=0.2))

        # Final output
        model.add(layers.Conv2DTranspose(self.img_shape[-1], 4, strides=2, padding='same', activation='tanh'))
        return model

    def build_discriminator(self):
        model = tf.keras.Sequential(name="Discriminator")
        
        model.add(layers.Conv2D(32, 4, strides=2, padding='same', input_shape=self.img_shape))
        model.add(layers.LeakyReLU(alpha=0.2))
        model.add(layers.Dropout(0.25))

        for filters in [64, 128, 256]:
            model.add(layers.Conv2D(filters, 4, strides=2, padding='same'))
            model.add(layers.BatchNormalization())
            model.add(layers.LeakyReLU(alpha=0.2))
            model.add(layers.Dropout(0.25))

        model.add(layers.Flatten())
        model.add(layers.Dense(1, activation='sigmoid'))
        return model

    def build_gan(self):
        self.discriminator.trainable = False
        z = layers.Input(shape=(self.latent_dim,))
        img = self.generator(z)
        validity = self.discriminator(img)
        model = Model(z, validity)
        model.compile(loss='binary_crossentropy', optimizer=self.optimizer)
        return model

    def save_sample_images(self, epoch, save_dir='images'):
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        r, c = 5, 5
        noise = np.random.normal(0, 1, (r * c, self.latent_dim))
        gen_imgs = self.generator.predict(noise)
        gen_imgs = 0.5 * gen_imgs + 0.5 # Rescale to [0, 1] for plot

        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                if self.img_shape[-1] == 1: # Grayscale
                    axs[i,j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')
                else:
                    axs[i,j].imshow(gen_imgs[cnt, :, :])
                axs[i,j].axis('off')
                cnt += 1
        fig.savefig(f"{save_dir}/epoch_{epoch}.png")
        plt.close()

# ==========================================
# 2. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ì‚¬ìš©ì ë°ì´í„° ì—°ê²°ë¶€)
# ==========================================
def load_data(img_shape):
    # TODO: ì—¬ê¸°ì— ì‹¤ì œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    # í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ëœë¤ ë…¸ì´ì¦ˆ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    print("âš ï¸ ê²½ê³ : ì‹¤ì œ ë°ì´í„°ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ìš© ëœë¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    X_train = np.random.normal(0, 1, (1000, img_shape[0], img_shape[1], img_shape[2]))
    
    # ë°ì´í„° ì •ê·œí™” (-1 ~ 1)
    # X_train = (X_train.astype(np.float32) - 127.5) / 127.5
    return X_train

# ==========================================
# 3. í•™ìŠµ ë° ì‹¤í–‰ ë¡œì§
# ==========================================
def train():
    # íŒŒë¼ë¯¸í„° íŒŒì‹± (ì™¸ë¶€ì—ì„œ ë³€ìˆ˜ ì¡°ì • ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •)
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=2000, help='ì´ í•™ìŠµ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--lr', type=float, default=0.0002, help='í•™ìŠµë¥  (Learning Rate)')
    parser.add_argument('--latent_dim', type=int, default=100, help='ì ì¬ ê³µê°„ ì°¨ì›')
    parser.add_argument('--interval', type=int, default=100, help='ì´ë¯¸ì§€ ì €ì¥ ë° ë¡œê·¸ ì¶œë ¥ ê°„ê²©')
    parser.add_argument('--save_dir', type=str, default='saved_models', help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ')
    args = parser.parse_args()

    # ì„¤ì •ê°’ ì¶œë ¥
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘! ì„¤ì •ê°’: {args}\n")

    # ë””ë ‰í† ë¦¬ ìƒì„±
    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)

    # ëª¨ë¸ ì´ˆê¸°í™”
    img_shape = (128, 128, 1) # í•„ìš”ì‹œ (128, 128, 3)ìœ¼ë¡œ ë³€ê²½
    gan = AnomalyGAN(img_shape, args.latent_dim, args.lr)
    
    # ë°ì´í„° ë¡œë“œ
    X_train = load_data(img_shape)

    # í•™ìŠµ ë£¨í”„ìš© ë³€ìˆ˜
    valid = np.ones((args.batch_size, 1))
    fake = np.zeros((args.batch_size, 1))
    
    # **ê°€ì¥ ì´ìƒì ì¸ ëª¨ë¸ì„ ì°¾ê¸° ìœ„í•œ ë³€ìˆ˜**
    best_g_loss = float('inf') 

    for epoch in range(args.epochs):
        # 1. Discriminator í•™ìŠµ
        idx = np.random.randint(0, X_train.shape[0], args.batch_size)
        imgs = X_train[idx]
        
        noise = np.random.normal(0, 1, (args.batch_size, args.latent_dim))
        gen_imgs = gan.generator.predict(noise)
        
        d_loss_real = gan.discriminator.train_on_batch(imgs, valid * 0.9) # Label Smoothing
        d_loss_fake = gan.discriminator.train_on_batch(gen_imgs, fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # 2. Generator í•™ìŠµ
        noise = np.random.normal(0, 1, (args.batch_size, args.latent_dim))
        g_loss = gan.gan.train_on_batch(noise, valid)

        # 3. ë¡œê¹… ë° Best Model ì €ì¥
        # ì´ìƒ íƒì§€ì—ì„œëŠ” G lossê°€ ë‚®ì€ ê²ƒì´ (ë³´í†µ) ì •ìƒ ë°ì´í„°ë¥¼ ì˜ í‰ë‚´ë‚¸ë‹¤ëŠ” ëœ»
        if g_loss < best_g_loss:
            best_g_loss = g_loss
            gan.generator.save(f"{args.save_dir}/best_generator.h5")
            print(f"Epoch {epoch}: ğŸ”¥ ìƒˆë¡œìš´ Best Model ì €ì¥ë¨! (G Loss: {g_loss:.4f})")

        if epoch % args.interval == 0:
            print(f"Epoch {epoch} [D loss: {d_loss[0]:.4f}] [G loss: {g_loss:.4f}]")
            gan.save_sample_images(epoch)

    # í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ëª¨ë¸ ì €ì¥
    gan.generator.save(f"{args.save_dir}/final_generator.h5")
    print("\nâœ… í•™ìŠµ ì™„ë£Œ! 'best_generator.h5'ì™€ 'final_generator.h5'ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    train()
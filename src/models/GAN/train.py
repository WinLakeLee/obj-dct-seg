import tensorflow as tf
from tensorflow.keras import layers, Model, optimizers
import numpy as np
import os
import json
import shutil
from datetime import datetime
import argparse
from .gan import AnomalyGAN
from pathlib import Path
import config

from common.data_utils import find_first_class, load_numpy_images
import matplotlib.pyplot as plt

# ==========================================
# 1. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (DCGAN êµ¬ì¡°)
# ==========================================


# ==========================================
# 2. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ì‚¬ìš©ì ë°ì´í„° ì—°ê²°ë¶€)
# ==========================================
def load_data(img_shape, data_dir=None, max_images=None):
    """
    data_dirì— ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ì—¬ (N, H, W, C) í˜•íƒœì˜ numpy ë°°ì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    - ì´ë¯¸ì§€ í¬ê¸°ëŠ” img_shapeì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆë©ë‹ˆë‹¤.
    - ì±„ë„ ìˆ˜ê°€ 1ì´ë©´ grayscaleë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - í”½ì…€ê°’ì€ -1 ~ 1 ë²”ìœ„ë¡œ ì •ê·œí™”ë©ë‹ˆë‹¤.
    """
    if not data_dir:
        print("âš ï¸ ê²½ê³ : --data_dirê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ìš© ëœë¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return np.random.normal(0, 1, (1000, img_shape[0], img_shape[1], img_shape[2]))

    try:
        return load_numpy_images(data_dir, img_shape, max_images=max_images)
    except RuntimeError as e:
        print(f"âš ï¸ ê²½ê³ : {e}. ëœë¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return np.random.normal(0, 1, (1000, img_shape[0], img_shape[1], img_shape[2]))


def collect_train_images(root, class_name, size=(128, 128), channels=1):
    """Load training images for a class under a given root using shared loader."""
    data_dir = Path(root) / class_name / 'train'
    img_shape = (size[0], size[1], channels)
    return load_numpy_images(data_dir, img_shape)

# ==========================================
# 3. í•™ìŠµ ë° ì‹¤í–‰ ë¡œì§
# ==========================================
def run_training(args):
    # ì„¤ì •ê°’ ì¶œë ¥
    print(f"\nğŸš€ í•™ìŠµ ì‹œì‘! ì„¤ì •ê°’: {args}\n")

    if args.seed is not None:
        tf.random.set_seed(args.seed)
        np.random.seed(args.seed)

    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)

    img_shape = (args.img_size, args.img_size, args.channels)
    gan = AnomalyGAN(img_shape, args.latent_dim, args.lr)
    # Ensure discriminator is compiled before calling `train_on_batch`.
    # AnomalyGAN builds `gan.gan` (generator+frozen discriminator) and optimizers,
    # but the standalone discriminator needs to be compiled for `train_on_batch`.
    try:
        gan.discriminator.compile(optimizer=gan.d_optimizer, loss=gan.bce)
    except Exception:
        # If compilation fails for any reason, print a helpful message and re-raise.
        print("Failed to compile discriminator; ensure optimizer and loss are valid.")
        raise
    X_train = load_data(img_shape, args.data_dir, args.max_images)

    valid = np.ones((args.batch_size, 1))
    fake = np.zeros((args.batch_size, 1))
    best_g_loss = float('inf')
    g_loss_history = []
    # advanced patience parameters
    patience = getattr(args, 'patience', 5)
    min_delta = getattr(args, 'min_delta', 1e-4)
    min_epochs = getattr(args, 'min_epochs', 10)
    stag_w = getattr(args, 'stagnation_window', 5)
    max_ratio = getattr(args, 'max_improve_ratio', 2.0)
    bonus_epochs = getattr(args, 'bonus_epochs_on_large_improve', 3)

    no_improve = 0
    bonus_remaining = 0

    epoch = 0
    max_epochs = getattr(args, 'epochs', 0)
    unlimited = (max_epochs <= 0)

    while unlimited or epoch < max_epochs:
        epoch += 1
        idx = np.random.randint(0, X_train.shape[0], args.batch_size)
        imgs = X_train[idx]

        noise = np.random.normal(0, 1, (args.batch_size, args.latent_dim))
        gen_imgs = gan.generator.predict(noise)

        d_loss_real = gan.discriminator.train_on_batch(imgs, valid * 0.9)
        d_loss_fake = gan.discriminator.train_on_batch(gen_imgs, fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        noise = np.random.normal(0, 1, (args.batch_size, args.latent_dim))
        g_loss = gan.gan.train_on_batch(noise, valid)

        improved = False
        g_loss_history.append(float(g_loss))

        # Direct improvement check (absolute improvement)
        if float(g_loss) + min_delta < best_g_loss:
            best_g_loss = float(g_loss)
            gan.generator.save(f"{args.save_dir}/best_generator.h5")
            print(f"Epoch {epoch}: ğŸ”¥ ìƒˆë¡œìš´ Best Model ì €ì¥ë¨! (G Loss: {float(g_loss):.6f})")
            improved = True
            no_improve = 0
        else:
            # If we haven't reached minimum epochs, don't count as failure
            if epoch < min_epochs:
                no_improve = 0
            else:
                # Sliding-window stagnation detection:
                # Compare the average G-loss of the previous window vs current window.
                # If current_avg + min_delta < prev_avg -> treat as improvement and reset counter.
                if len(g_loss_history) >= 2 * stag_w and stag_w > 0:
                    prev_avg = float(np.mean(g_loss_history[-2 * stag_w:-stag_w]))
                    curr_avg = float(np.mean(g_loss_history[-stag_w:]))
                    if curr_avg + min_delta < prev_avg:
                        # improvement detected in the sliding window
                        no_improve = 0
                        # Large improvement handling: grant bonus epochs
                        if prev_avg / max(curr_avg, 1e-12) > max_ratio:
                            print(f"Epoch {epoch}: Large improvement detected (ratio {prev_avg/curr_avg:.2f}), granting bonus {bonus_epochs} epochs")
                            bonus_remaining = max(bonus_remaining, bonus_epochs)
                    else:
                        # no improvement within the sliding window -> increment
                        no_improve += 1
                else:
                    # Not enough history yet -> increment conservatively
                    no_improve += 1

        # Consume bonus if present (bonus prevents early stopping while positive)
        if bonus_remaining > 0:
            effective_no_improve = 0
            bonus_remaining -= 1
        else:
            effective_no_improve = no_improve

        # Always print a short epoch summary so `no_improve` visibility is immediate.
        try:
            d_loss_val = float(d_loss[0])
        except Exception:
            d_loss_val = float(d_loss)
        g_loss_val = float(g_loss)
        # diagnostic info for sliding-window when available
        win_info = ""
        if stag_w > 0 and len(g_loss_history) >= 2 * stag_w:
            prev_avg = float(np.mean(g_loss_history[-2 * stag_w:-stag_w]))
            curr_avg = float(np.mean(g_loss_history[-stag_w:]))
            win_info = f" prev_avg={prev_avg:.6f} curr_avg={curr_avg:.6f}"

        # Short status printed every epoch (diagnostic: show G loss, best, and whether it improved)
        print(f"Epoch {epoch} [no_improve: {no_improve}/{patience}] [effective_no_improve: {effective_no_improve}] [G: {g_loss_val:.6f}] [best: {best_g_loss:.6f}] [imp: {'Y' if improved else 'N'}]{win_info}")

        # Detailed logging and sample saving still follow the original rules
        if epoch % args.interval == 0 or improved:
            print(f"Epoch {epoch} [D loss: {d_loss_val:.6f}] [G loss: {g_loss_val:.6f}] [best: {best_g_loss:.6f}] [bonus_left: {bonus_remaining}]")
            try:
                gan.save_sample_images(args.save_dir, epoch)
            except Exception as e:
                print(f"ìƒ˜í”Œ ì €ì¥ ì‹¤íŒ¨: {e}")

        # Early stopping: require minimum epochs and check effective no_improve
        if epoch >= min_epochs and effective_no_improve >= patience:
            print(f"â±ï¸ Early stopping triggered at epoch {epoch} (no improvement for {effective_no_improve} epochs, patience {patience}).")
            break

    final_model_path = f"{args.save_dir}/final_generator.h5"
    gan.generator.save(final_model_path)
    # If recon export requested, also save a run-local recon model
    export_reconstruction_model(gan.generator, X_train, os.path.join(args.save_dir, 'best_reconstructor.h5'), epochs=getattr(args, 'export_recon_epochs', 0), batch_size=args.batch_size)

    # Optional: build an image->image recon model (encoder + frozen generator) for inference.
    # This trains only the encoder for a few epochs to map images to the generator's latent space.
    def export_reconstruction_model(generator, train_data, save_path, epochs=0, batch_size=16):
        if epochs <= 0:
            return None
        import tensorflow as tf
        from tensorflow.keras import layers, Model
        encoder_input = layers.Input(shape=img_shape)
        x = encoder_input
        x = layers.Conv2D(64, 4, strides=2, padding='same', activation='leaky_relu')(x)
        x = layers.Conv2D(128, 4, strides=2, padding='same', activation='leaky_relu')(x)
        x = layers.Conv2D(256, 4, strides=2, padding='same', activation='leaky_relu')(x)
        x = layers.Conv2D(256, 4, strides=2, padding='same', activation='leaky_relu')(x)
        x = layers.Flatten()(x)
        latent = layers.Dense(args.latent_dim)(x)

        generator.trainable = False
        recon = generator(latent)
        recon_model = Model(encoder_input, recon, name='EncoderGeneratorRecon')
        recon_model.compile(optimizer=tf.keras.optimizers.Adam(args.lr), loss='mae')
        recon_model.fit(train_data, train_data, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True)
        recon_model.save(save_path)
        return save_path

    # í‰ê°€ ë° ê¸€ë¡œë²Œ ë² ìŠ¤íŠ¸ì™€ ë¹„êµ
    # í˜„ì¬ ì§€í‘œ: ì´ ì‹¤í–‰ì—ì„œì˜ best generator loss (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
    current_metric = best_g_loss if best_g_loss != float('inf') else float('inf')

    global_best_file = os.path.join('outputs', 'global_gan_best.json')
    os.makedirs('outputs', exist_ok=True)

    keep_run = True
    if os.path.exists(global_best_file):
        with open(global_best_file, 'r') as f:
            data = json.load(f)
        best_metric = data.get('best_metric', float('inf'))
        if current_metric < best_metric:
            # ë” ì¢‹ìŒ -> ê°±ì‹ 
            shutil.copyfile(f"{args.save_dir}/best_generator.h5", os.path.join('outputs', 'global_best_generator.h5'))
            # export recon model if requested
            export_reconstruction_model(gan.generator, X_train, os.path.join('outputs', 'global_best_reconstructor.h5'), epochs=getattr(args, 'export_recon_epochs', 0), batch_size=args.batch_size)
            data = {
                'best_metric': current_metric,
                'saved_at': datetime.utcnow().isoformat(),
                'source_dir': args.save_dir
            }
            with open(global_best_file, 'w') as f:
                json.dump(data, f)
            print(f"âœ… Run improved global best (metric {current_metric:.6f}). Global best updated.")
        else:
            # ì¢‹ì§€ ì•ŠìŒ -> ì‚­ì œ ì˜µì…˜ì´ ì¼œì ¸ ìˆìœ¼ë©´ ê²°ê³¼ ì œê±°
            if getattr(args, 'prune_if_worse', True):
                try:
                    shutil.rmtree(args.save_dir)
                    print(f"ğŸ—‘ï¸ Run did not beat global best (current {current_metric:.6f} >= best {best_metric:.6f}). Deleted {args.save_dir}.")
                    keep_run = False
                except Exception as e:
                    print(f"Failed to remove {args.save_dir}: {e}")
            else:
                print(f"Run did not beat global best (current {current_metric:.6f} >= best {best_metric:.6f}). Kept artifacts.")
    else:
        # ì²˜ìŒ ë² ìŠ¤íŠ¸ ì„¤ì •
        shutil.copyfile(f"{args.save_dir}/best_generator.h5", os.path.join('outputs', 'global_best_generator.h5'))
        export_reconstruction_model(gan.generator, X_train, os.path.join('outputs', 'global_best_reconstructor.h5'), epochs=getattr(args, 'export_recon_epochs', 0), batch_size=args.batch_size)
        data = {
            'best_metric': current_metric,
            'saved_at': datetime.utcnow().isoformat(),
            'source_dir': args.save_dir
        }
        with open(global_best_file, 'w') as f:
            json.dump(data, f)
        print(f"âœ… No previous global best. Saved current run as global best (metric {current_metric:.6f}).")

    if keep_run:
        print("\nâœ… í•™ìŠµ ì™„ë£Œ! 'best_generator.h5'ì™€ 'final_generator.h5'ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def train():
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['dcgan','anomaly'], default='dcgan', help='í•™ìŠµ ëª¨ë“œ ì„ íƒ')
    parser.add_argument('--epochs', type=int, default=2000, help='ì´ í•™ìŠµ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ ì‚¬ì´ì¦ˆ')
    parser.add_argument('--lr', type=float, default=0.0002, help='í•™ìŠµë¥  (Learning Rate)')
    parser.add_argument('--latent_dim', type=int, default=100, help='ì ì¬ ê³µê°„ ì°¨ì›')
    parser.add_argument('--interval', type=int, default=100, help='ì´ë¯¸ì§€ ì €ì¥ ë° ë¡œê·¸ ì¶œë ¥ ê°„ê²©')
    parser.add_argument('--save_dir', type=str, default=None, help='ëª¨ë¸ ì €ì¥ ê²½ë¡œ (env SAVE_DIR ë˜ëŠ” config.get_save_dir)')
    parser.add_argument('--data_dir', type=str, default=None, help='í•™ìŠµ ì´ë¯¸ì§€ í´ë” (config.get_data_paths train_dir ì‚¬ìš©)')
    parser.add_argument('--img_size', type=int, default=128, help='ì •ì‚¬ê° ì´ë¯¸ì§€ í¬ê¸°')
    parser.add_argument('--channels', type=int, default=1, help='ì±„ë„ ìˆ˜ (1=gray, 3=RGB)')
    parser.add_argument('--seed', type=int, default=None, help='ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ')
    parser.add_argument('--max_images', type=int, default=None, help='í•™ìŠµì— ì‚¬ìš©í•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì œí•œ')
    # anomaly specific
    parser.add_argument('--class', dest='class_name', default=None, help='MVTec class name for anomaly mode (env CLASS_NAME fallback)')
    parser.add_argument('--mvtec_root', type=str, default=None, help='MVTec root folder (env MVTEC_ROOT fallback)')
    parser.add_argument('--dry_run', action='store_true', help='ë°ì´í„° ë¡œë“œë§Œ í™•ì¸í•˜ê³  ì¢…ë£Œ')
    parser.add_argument('--patience', type=int, default=5, help='Early stopping patience (epochs)')
    parser.add_argument('--min_delta', type=float, default=1e-4, help='Minimum absolute improvement to count')
    parser.add_argument('--min_epochs', type=int, default=10, help='Minimum epochs before early stopping allowed')
    parser.add_argument('--stagnation_window', type=int, default=5, help='Window size for stagnation detection (epochs)')
    parser.add_argument('--max_improve_ratio', type=float, default=2.0, help='If avg(prev_window)/avg(curr_window) > this, treat as large improvement')
    parser.add_argument('--bonus_epochs_on_large_improve', type=int, default=3, help='Extra patience epochs after a large improvement')
    parser.add_argument('--prune_if_worse', type=bool, default=True, help='If True, delete run artifacts when not improving global best')
    parser.add_argument('--export_recon_epochs', type=int, default=0, help='If >0, train a lightweight encoder+frozen-generator recon model for this many epochs and save as best_reconstructor.h5 (and global_best_reconstructor.h5 when improved)')

    args = parser.parse_args()

    # Resolve shared paths via config
    if not args.class_name:
        args.class_name = config.DATA_CLASS
    if not args.mvtec_root:
        args.mvtec_root = str(config.DATA_ORIGIN)
    if not args.data_dir:
        train_dir, _ = config.get_data_paths(args.class_name)
        args.data_dir = str(train_dir)
    if not args.save_dir:
        args.save_dir = str(config.get_save_dir('saved_models'))

    if args.mode in ('dcgan', 'anomaly'):
        # For backward compatibility, call existing run_training for the DCGAN-style AnomalyGAN
        if args.mode == 'anomaly':
            # prefer MVTec layout if available
            imgs = collect_train_images(args.mvtec_root, args.class_name or find_first_class(args.mvtec_root), size=(args.img_size,args.img_size))
            if args.dry_run:
                print(f"Dry run: loaded {imgs.shape[0]} images for class {args.class_name}")
                return
            os.makedirs(args.save_dir, exist_ok=True)
            agan = AnomalyGAN((args.img_size,args.img_size,args.channels), args.latent_dim, args.lr)
            agan.train = None  # avoid attribute collision; use existing run_training
            # reuse run_training by constructing a minimal args namespace
            class SimpleArgs:
                pass
            ra = SimpleArgs()
            ra.epochs = args.epochs
            ra.batch_size = args.batch_size
            ra.lr = args.lr
            ra.latent_dim = args.latent_dim
            ra.interval = args.interval
            ra.save_dir = args.save_dir
            ra.data_dir = args.data_dir
            ra.img_size = args.img_size
            ra.channels = args.channels
            ra.seed = args.seed
            ra.max_images = args.max_images
            run_training(ra)
        else:
            run_training(args)

if __name__ == '__main__':
    train()
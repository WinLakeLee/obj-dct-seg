import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from sklearn.neighbors import NearestNeighbors
from sklearn.random_projection import SparseRandomProjection
import numpy as np
from torchvision import transforms
import json
from pathlib import Path

# ---------------------------------------------------------
# 1. ì„±ëŠ¥ ìµœì í™”: TF32 í™œì„±í™” (Ampere GPU ì´ìƒì—ì„œ ì†ë„ í–¥ìƒ)
# ---------------------------------------------------------
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# torchvision ìµœì‹  ë²„ì „ í˜¸í™˜
try:
    from torchvision.models import Wide_ResNet50_2_Weights

    _WIDE_RESNET_WEIGHTS = Wide_ResNet50_2_Weights.DEFAULT
except Exception:
    _WIDE_RESNET_WEIGHTS = None

# FAISS ìœ ë¬´ í™•ì¸
try:
    import faiss

    HAS_FAISS = True
except Exception:
    HAS_FAISS = False

# Triton availability (required for torch.compile -> inductor backend)
try:
    import triton  # noqa: F401

    HAS_TRITON = True
except Exception:
    HAS_TRITON = False

logger = logging.getLogger(__name__)


class PatchCoreOptimized:
    def __init__(
        self, backbone_name="wide_resnet50_2", sampling_ratio=0.01, use_fp16=True
    ):
        """
        Args:
            sampling_ratio (float): ë©”ëª¨ë¦¬ ë±…í¬ ìƒ˜í”Œë§ ë¹„ìœ¨.
            use_fp16 (bool): Trueì¼ ê²½ìš° FP16(ë°˜ì •ë°€ë„) ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ë„ í–¥ìƒ ë° ë©”ëª¨ë¦¬ ì ˆì•½.
        """
        self.sampling_ratio = sampling_ratio
        self.use_fp16 = use_fp16 and torch.cuda.is_available()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # 1. Backbone ë¡œë“œ
        if backbone_name == "wide_resnet50_2":
            if _WIDE_RESNET_WEIGHTS is not None:
                self.backbone = models.wide_resnet50_2(weights=_WIDE_RESNET_WEIGHTS)
            else:
                self.backbone = models.wide_resnet50_2(pretrained=True)
        else:
            self.backbone = models.resnet18(pretrained=True)

        self.backbone.eval()
        self.backbone.to(self.device)

        # ---------------------------------------------------------
        # 2. ì„±ëŠ¥ ìµœì í™”: FP16 ëª¨ë“œ (ë©”ëª¨ë¦¬ ì ˆë°˜, ì†ë„ ì¦ê°€)
        # ---------------------------------------------------------
        if self.use_fp16:
            self.backbone.half()
            logger.info("ðŸš€ FP16(Half Precision) ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ---------------------------------------------------------
        # 3. ì„±ëŠ¥ ìµœì í™”: torch.compile (PyTorch 2.x ì´ìƒ, triton í•„ìš”)
        # ---------------------------------------------------------
        if hasattr(torch, "compile") and HAS_TRITON:
            try:
                self.backbone = torch.compile(self.backbone)
                logger.info("ðŸš€ PyTorch 2.0 Compilationì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"Compilation ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
        else:
            logger.info("torch.compile ê±´ë„ˆëœ€ (triton ì—†ìŒ ë˜ëŠ” í™˜ê²½ ë¯¸ì§€ì›)")

        # íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ Hook ì„¤ì •
        self.features = []
        self._register_hooks()

        self.memory_bank = None
        self.knn = None
        self.faiss_index = None
        self.n_neighbors = 9

    def to(self, device):
        """Move backbone and update internal device tracking."""
        self.device = torch.device(device)
        self.backbone.to(self.device)
        return self

    def _hook_fn(self, module, input, output):
        # FP16 ëª¨ë“œì¼ ê²½ìš° Hook ì¶œë ¥ë„ FP16ì¼ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ í•„ìš”ì‹œ ì²˜ë¦¬ ê°€ëŠ¥
        self.features.append(output)

    def _register_hooks(self):
        self.backbone.layer2.register_forward_hook(self._hook_fn)
        self.backbone.layer3.register_forward_hook(self._hook_fn)

    def extract_features(self, x):
        """ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ìž…ë ¥ë°›ì•„ (N_patches, Dim) í˜•íƒœì˜ íŠ¹ì§• ë²¡í„° ë°˜í™˜"""
        self.features = []

        # ìž…ë ¥ ë°ì´í„° ìž¥ì¹˜ ë° íƒ€ìž… ë³€í™˜
        x = x.to(self.device)
        if self.use_fp16:
            x = x.half()

        with torch.no_grad():
            self.backbone(x)

        # Feature Map ê°€ì ¸ì˜¤ê¸°
        f2 = self.features[0]
        f3 = self.features[1]

        # Upsampling & Concatenation
        # F.interpolateëŠ” FP16ì—ì„œ ë™ìž‘í•˜ì§€ë§Œ, ì•ˆì •ì„±ì„ ìœ„í•´ float32ë¡œ ë³€í™˜í•´ì„œ ê³„ì‚°í•˜ëŠ” ê²½ìš°ë„ ìžˆìŒ.
        # ì—¬ê¸°ì„œëŠ” ì†ë„ë¥¼ ìœ„í•´ ê·¸ëŒ€ë¡œ ì§„í–‰í•˜ë˜ align_corners=TrueëŠ” ìœ ì§€
        f3_resized = F.interpolate(
            f3, size=f2.shape[-2:], mode="bilinear", align_corners=True
        )
        concat_features = torch.cat([f2, f3_resized], dim=1)

        # Average Pooling (Smoothing)
        patch_features = F.avg_pool2d(
            concat_features, kernel_size=3, stride=1, padding=1
        )

        # (Batch, C, H, W) -> (Batch, H, W, C) -> (N, C)
        patch_features = patch_features.permute(0, 2, 3, 1)
        output_features = patch_features.reshape(-1, patch_features.shape[-1])

        # ì£¼ì˜: Faiss(CPU)ë‚˜ Sklearnì€ float32ë§Œ ë°›ìŠµë‹ˆë‹¤.
        # ë”°ë¼ì„œ ë°˜í™˜ ì‹œì—ëŠ” float32ë¡œ ìºìŠ¤íŒ…í•˜ì—¬ CPUë¡œ ë³´ëƒ…ë‹ˆë‹¤.
        return output_features.float().cpu()

    @staticmethod
    def get_train_transforms(
        resize_size=256,
        crop_size=224,
        random_crop=False,
        hflip=False,
        rotation=0.0,
        color_jitter=0.0,
    ):
        """PatchCore í•™ìŠµìš© ë°ì´í„° ì¦ê°• íŒŒì´í”„ë¼ì¸."""
        tfs = [transforms.Resize(resize_size)]
        if random_crop:
            tfs.append(transforms.RandomCrop(crop_size))
        else:
            tfs.append(transforms.CenterCrop(crop_size))
        if hflip:
            tfs.append(transforms.RandomHorizontalFlip(p=0.5))
        if rotation and rotation > 0:
            tfs.append(transforms.RandomRotation(degrees=rotation))
        if color_jitter and color_jitter > 0:
            tfs.append(
                transforms.ColorJitter(
                    brightness=color_jitter,
                    contrast=color_jitter,
                    saturation=color_jitter / 2,
                )
            )
        tfs.extend(
            [
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
                ),
            ]
        )
        return transforms.Compose(tfs)

    def predict_tta(self, img, score_type="max", kneighbors_batch=None):
        """Test-Time Augmentation ê¸°ë°˜ í‰ê·  ì ìˆ˜ ê³„ì‚°."""
        augmented_imgs = [
            img,
            transforms.functional.hflip(img),
            transforms.functional.rotate(img, angle=5),
            transforms.functional.rotate(img, angle=-5),
        ]

        scores_list = []
        for aug_img in augmented_imgs:
            score = self.predict(
                aug_img, score_type=score_type, kneighbors_batch=kneighbors_batch
            )
            scores_list.append(score[0])

        final_score = np.mean(scores_list)
        return [final_score]

    def _compute_greedy_coreset_indices(
        self, features: np.ndarray, sampling_ratio: float
    ) -> np.ndarray:
        """
        PatchCoreì˜ í•µì‹¬: K-Center Greedy ì•Œê³ ë¦¬ì¦˜
        ë¬´ìž‘ìœ„ê°€ ì•„ë‹ˆë¼, ê°€ìž¥ ìœ ì˜ë¯¸í•œ(ê±°ë¦¬ê°€ ë¨¼) íŠ¹ì§•ë“¤ì„ ê³¨ë¼ëƒ…ë‹ˆë‹¤.
        """
        sample_size = int(features.shape[0] * sampling_ratio)
        if sample_size >= features.shape[0]:
            return np.arange(features.shape[0])

        logger.info(
            f"ðŸ§  Coreset Sampling ì‹œìž‘: {features.shape[0]} -> {sample_size} (ì •í™•ë„ í–¥ìƒ ì¤‘...)"
        )

        # 1. ì†ë„ë¥¼ ìœ„í•´ Random Projectionìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ (ì˜ˆ: 1024 -> 128)
        # ì°¨ì›ì´ ì¤„ì–´ë„ ì ë“¤ ê°„ì˜ ê±°ë¦¬ ë¹„ìœ¨ì€ ìœ ì§€ëœë‹¤ëŠ” ì¡´ìŠ¨-ë¦°ë´ìŠˆíŠ¸ë¼ìš°ìŠ¤ ë³´ì¡°ì •ë¦¬ í™œìš©
        reducer = SparseRandomProjection(n_components="auto", eps=0.9)
        reduced_features = reducer.fit_transform(features)

        # 2. Greedy Selection
        # ì²« ë²ˆì§¸ ì ì€ ë¬´ìž‘ìœ„ ì„ íƒ
        selector = [np.random.randint(features.shape[0])]
        selected_indices = [selector[0]]

        # ê°€ìž¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ê¹Œì§€ì˜ ê±°ë¦¬ ì €ìž¥
        # ì´ˆê¸°ì—ëŠ” ì²« ë²ˆì§¸ ì„ íƒëœ ì ê³¼ì˜ ê±°ë¦¬ë¡œ ì´ˆê¸°í™”
        dist_matrix = np.linalg.norm(
            reduced_features - reduced_features[selector[0]], axis=1
        )

        for _ in range(1, sample_size):
            # í˜„ìž¬ ì„ íƒëœ ì ë“¤ë¡œë¶€í„° ê°€ìž¥ 'ë©€ë¦¬' ìžˆëŠ” ì ì„ ë‹¤ìŒ ì ìœ¼ë¡œ ì„ íƒ
            # (ê°€ìž¥ ìž˜ ëŒ€ë³€ë˜ì§€ ì•Šì€ ì˜ì—­ì„ ì»¤ë²„í•˜ê¸° ìœ„í•´)
            next_index = np.argmax(dist_matrix)

            # ì„ íƒëœ ì  ì¶”ê°€
            selected_indices.append(next_index)

            # ê±°ë¦¬ ê°±ì‹ : ê¸°ì¡´ ê±°ë¦¬ vs ìƒˆë¡œ ì„ íƒëœ ì ê³¼ì˜ ê±°ë¦¬ ì¤‘ ë” ìž‘ì€ ê°’ ìœ ì§€
            new_dist = np.linalg.norm(
                reduced_features - reduced_features[next_index], axis=1
            )
            dist_matrix = np.minimum(dist_matrix, new_dist)

        return np.array(selected_indices)

    def fit(
        self,
        train_loader,
        n_neighbors=9,
        checkpoint_dir=None,
        checkpoint_interval=None,
    ):
        logger.info("ðŸ§  í•™ìŠµ ì‹œìž‘: íŠ¹ì§• ì¶”ì¶œ ë° ë©”ëª¨ë¦¬ ë±…í¬ êµ¬ì¶•...")
        features_list = []

        # ë°°ì¹˜ ë‹¨ìœ„ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        for step, imgs in enumerate(train_loader, start=1):
            feats = self.extract_features(imgs)
            features_list.append(feats.numpy())

            if checkpoint_interval and step % checkpoint_interval == 0:
                logger.info("Processed %d batches so far", step)

        # 1. ì „ì²´ íŠ¹ì§• í•©ì¹˜ê¸°
        full_bank = np.concatenate(features_list, axis=0)

        # ---------------------------------------------------------
        # [ìˆ˜ì •] 2. ì„±ëŠ¥ ìµœì í™”: Random -> Coreset Sampling ë³€ê²½
        # ---------------------------------------------------------
        if self.sampling_ratio < 1.0:
            indices = self._compute_greedy_coreset_indices(
                full_bank, self.sampling_ratio
            )
            self.memory_bank = full_bank[indices]
        else:
            self.memory_bank = full_bank

        self.memory_bank = np.ascontiguousarray(self.memory_bank.astype(np.float32))
        self.n_neighbors = n_neighbors
        self._build_index()

        if checkpoint_dir:
            self._save_checkpoint(checkpoint_dir)

    def _save_checkpoint(self, checkpoint_dir):
        ckpt_dir = Path(checkpoint_dir)
        ckpt_dir.mkdir(parents=True, exist_ok=True)

        mb_path = ckpt_dir / "memory_bank.npy"
        np.save(str(mb_path), self.memory_bank)

        meta = {
            "sampling_ratio": self.sampling_ratio,
            "n_neighbors": self.n_neighbors,
            "use_fp16": self.use_fp16,
            "faiss": self.faiss_index is not None,
        }
        with open(ckpt_dir / "meta.json", "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2)

        if self.knn is not None:
            try:
                import joblib

                joblib.dump(self.knn, str(ckpt_dir / "knn.pkl"))
            except Exception as e:
                logger.warning("KNN ì €ìž¥ ì‹¤íŒ¨ (ë¬´ì‹œ ê°€ëŠ¥): %s", e)

    def _build_index(self):
        """KNN ë˜ëŠ” Faiss ì¸ë±ìŠ¤ ë¹Œë“œ"""
        dim = self.memory_bank.shape[1]

        if HAS_FAISS:
            # ---------------------------------------------------------
            # 4. ì„±ëŠ¥ ìµœì í™”: FAISS IndexFactory ì‚¬ìš© (ìžë™ ìµœì í™”)
            # ---------------------------------------------------------
            # ë°ì´í„°ê°€ ë§¤ìš° ë§Žë‹¤ë©´ 'IVF1024,Flat' ë“±ì„ ì‚¬ìš©í•˜ì—¬ ê·¼ì‚¬ ê²€ìƒ‰(ì†ë„â†‘) ê°€ëŠ¥
            # ì—¬ê¸°ì„œëŠ” ì •í™•ë„ë¥¼ ìœ„í•´ FlatL2ë¥¼ ì“°ë˜ GPU ìžì›ì„ í™œìš©
            index_str = "Flat"

            try:
                # GPU ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì‹œë„
                res = faiss.StandardGpuResources()
                # ì¸ë±ìŠ¤ ìƒì„±
                index = faiss.index_factory(dim, index_str, faiss.METRIC_L2)

                # GPUë¡œ ì´ë™ (ë©”ëª¨ë¦¬ê°€ í—ˆìš©í•˜ëŠ” ê²½ìš°)
                if torch.cuda.is_available():
                    index = faiss.index_cpu_to_gpu(res, 0, index)
                    logger.info("ðŸš€ FAISS: GPU ì¸ë±ì‹± ì„±ê³µ")

                index.add(self.memory_bank)
                self.faiss_index = index

            except Exception as e:
                logger.warning(f"FAISS GPU ì„¤ì • ì‹¤íŒ¨ ({e}). CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.faiss_index = faiss.IndexFlatL2(dim)
                self.faiss_index.add(self.memory_bank)
        else:
            logger.info("Faiss ì—†ìŒ: Scikit-Learn KNN ì‚¬ìš©.")
            self.knn = NearestNeighbors(n_neighbors=self.n_neighbors)
            self.knn.fit(self.memory_bank)

    def predict(self, img, score_type="max", kneighbors_batch=None):
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        if img.dim() == 3:
            img = img.unsqueeze(0)

        # íŠ¹ì§• ì¶”ì¶œ
        test_feat = self.extract_features(img).numpy()
        test_feat = np.ascontiguousarray(test_feat.astype(np.float32))

        # ê²€ìƒ‰
        if self.faiss_index is not None:
            distances = []
            if kneighbors_batch:
                for start in range(0, test_feat.shape[0], kneighbors_batch):
                    end = start + kneighbors_batch
                    D, _ = self.faiss_index.search(
                        test_feat[start:end], self.n_neighbors
                    )
                    distances.append(D)
                D = np.concatenate(distances, axis=0)
            else:
                D, _ = self.faiss_index.search(test_feat, self.n_neighbors)
            patch_scores = np.mean(D, axis=1)
        elif self.knn is not None:
            distances = []
            if kneighbors_batch:
                for start in range(0, test_feat.shape[0], kneighbors_batch):
                    end = start + kneighbors_batch
                    D, _ = self.knn.kneighbors(
                        test_feat[start:end], n_neighbors=self.n_neighbors
                    )
                    distances.append(D)
                D = np.concatenate(distances, axis=0)
            else:
                D, _ = self.knn.kneighbors(test_feat)
            patch_scores = np.mean(D, axis=1)
        else:
            raise RuntimeError("ëª¨ë¸ ë¯¸í•™ìŠµ ìƒíƒœ")

        # ë°°ì¹˜ë³„ ì ìˆ˜ ê³„ì‚°
        patches_per_img = test_feat.shape[0] // img.shape[0]
        batch_scores = []

        for i in range(img.shape[0]):
            start = i * patches_per_img
            end = (i + 1) * patches_per_img
            scores_in_img = patch_scores[start:end]

            if score_type == "max":
                score = np.max(scores_in_img)
            else:
                score = np.mean(scores_in_img)
            batch_scores.append(float(score))

        return batch_scores


# Backward-compatible alias used by training script
class PatchCoreFromScratch(PatchCoreOptimized):
    pass
